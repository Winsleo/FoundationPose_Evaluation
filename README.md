## Algorithm Overview
![Framework](images/framework.png)

The principle can be summarized as "render and compare": render an image given the reference model and a pose hypothesis, then compare it with the observed image to derive pose corrections.
1. Pose hypothesis generation: Given an RGB-D image, detect the object using methods such as Mask R-CNN. Initialize translation with the median depth inside the 2D bounding box, and initialize rotation by uniformly sampling viewpoints on an icosphere centered at the object.
2. Pose refinement: Given the initial pose and the observation cropped according to that pose, compare the rendered result with the observation to output pose updates.
3. Pose selection: Use a hierarchical comparison network to score all pose hypotheses with global context, and choose the highest-scoring pose as the final estimate.


## How to Run
1. Environment setup: please refer to [FoundationPose](https://github.com/NVlabs/FoundationPose). Using Docker is recommended.
2. Data preparation:
   - Camera intrinsics
   - RGB-D video stream
   - First-frame object segmentation mask (can be generated by [Grounded-SAM](https://github.com/IDEA-Research/Grounded-Segment-Anything))
   - Reference object model (Mesh)

   Example directory layout:
   ```
   ├── cam_K.txt
   ├── rgb
   │   ├── 1581120424100262102.png
   │   ├── 1581120424148532296.png
   │   ├── 1581120424213125141.png
   │   ├── 1581120424269362478.png
   │   ├── ...
   ├── depth
   │   ├── 1581120424100262102.png
   │   ├── 1581120424148532296.png
   │   ├── 1581120424213125141.png
   │   ├── 1581120424269362478.png
   │   ├── ...
   ├── mask
   │   ├── 1581120424100262102.png
   ├── mesh
   │   ├── textured_simple.obj
   │   ├── textured_simple.obj.mtl
   │   └── texture_map.png
   ```
3. Run:
   ```bash
   python run_linemod.py --linemod_dir <data_dir> --use_reconstructed_mesh 0
   ```


## One Way to Obtain a Reference Model
Generate a CAD model using [Tencent Hunyuan Image-to-3D](https://3d.hunyuan.tencent.com/) (capture front/back/left/right views of the object, and keep the background visually rich):
<p align="center"><b>CAD models generated by Tencent Hunyuan Image-to-3D</b></p>
<p align="center">
  <img src="images/image_to_3D1.png" alt="CAD-from-image-1" width="32%">
  <img src="images/image_to_3D2.png" alt="CAD-from-image-2" width="28%">
  <!-- more images may exist in the original doc -->
  <!-- ... -->
  <!-- end of truncated content note -->
  
</p>

The CAD model generated by Image-to-3D is normalized in scale. For better results, resize it to the real-world size:
```bash
python mesh_utils.py -i input_mesh.obj -o output_mesh.obj --size <size along the smallest OBB extent(meters)>
```


## Effect of Reference Model Size

<div align="center">
  <figure style="display:inline-block;margin:0 8px;text-align:center;">
    <img src="images/uniformed_size.gif" alt="uniformed_size" width="60%">
    <figcaption>Input: normalized reference mesh (no real-world scale)</figcaption>
  </figure>
  <figure style="display:inline-block;margin:0 8px;text-align:center;">
    <img src="images/real_size.gif" alt="real_size" width="60%">
    <figcaption>Input: reference mesh with real-world scale</figcaption>
  </figure>
  
</div>

**Conclusion**: The metric accuracy of the reference model's size has a significant impact on pose estimation performance and stability.


## Interference from Similar or Identical Objects

<div align="center">
  <figure style="display:inline-block;margin:0 8px;text-align:center;">
    <img src="images/no_occlusion_similar.gif" alt="no_occlusion_similar" width="60%">
    <figcaption>Similar objects</figcaption>
  </figure>
  <figure style="display:inline-block;margin:0 8px;text-align:center;">
    <img src="images/no_occlusion_same.gif" alt="no_occlusion_same" width="60%">
    <figcaption>Identical objects</figcaption>
  </figure>
  
</div>

**Conclusion**: With no occlusion, similar/identical objects in the field of view have negligible impact on performance.

## Effect of Occlusion

<div align="center">
  <figure style="display:inline-block;margin:0 6px;text-align:center;">
    <img src="images/occlusion_similar.gif" alt="occlusion_similar" width="60%">
    <figcaption>Occlusion by similar objects</figcaption>
  </figure>
  <figure style="display:inline-block;margin:0 6px;text-align:center;">
    <img src="images/occlusion_same1.gif" alt="occlusion_same1" width="60%">
    <figcaption>Occlusion by identical objects (1)</figcaption>
  </figure>
  <figure style="display:inline-block;margin:0 6px;text-align:center;">
    <img src="images/occlusion_same2.gif" alt="occlusion_same2" width="60%">
    <figcaption>Occlusion by identical objects (2)</figcaption>
  </figure>
</div>


**Conclusion**: For both similar and identical objects, short-term occlusions have comparable effects. FoundationPose shows certain robustness to short-term occlusions, but prolonged occlusions can lead to failure.


